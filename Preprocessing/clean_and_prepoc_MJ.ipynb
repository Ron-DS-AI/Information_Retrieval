{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d78a918e",
   "metadata": {},
   "source": [
    "### Open file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78f6e606-9adc-4975-bee1-d7a5a1534b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9318\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "import ftfy  \n",
    "from langdetect import detect\n",
    "import numpy as np \n",
    "\n",
    "# open metadata sample file\n",
    "\n",
    "metadata = pd.read_csv(\n",
    "    'purposes_metadata_final.csv',  \n",
    "    dtype={\n",
    "        'cord_uid': 'string',\n",
    "        'sha': 'string',\n",
    "        'source_x': 'string',\n",
    "        'title': 'string',\n",
    "        'doi': 'string',\n",
    "        'pmcid': 'string',\n",
    "        'pubmed_id': 'Int64',  \n",
    "        'license': 'string',\n",
    "        'abstract': 'string',\n",
    "        'publish_time': 'string',  \n",
    "        'authors': 'string',\n",
    "        'journal': 'string',\n",
    "        'mag_id': 'string',  \n",
    "        'who_covidence_id': 'string',  \n",
    "        'arxiv_id': 'string',  \n",
    "        'pdf_json_files': 'string',\n",
    "        'pmc_json_files': 'string',\n",
    "        'url': 'string',\n",
    "        's2_id': 'string',\n",
    "        'referenced_by_count':'int64', \n",
    "        'JournalName_DOI':'string', \n",
    "        'tags':'string',\n",
    "        'TagCount':'int64'\n",
    "    }, encoding='utf-8', low_memory=False\n",
    ")\n",
    "\n",
    "print(len(metadata))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e573441a",
   "metadata": {},
   "source": [
    "### Clean and normalise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c77503c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original rows: 9318\n",
      "Rows after English filter: 9293\n",
      "Rows after cleaning: 9293\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Drop duplicates\n",
    "metadata = metadata.drop_duplicates(subset=['cord_uid'])\n",
    "original_rows = len(metadata)\n",
    "\n",
    "# Fix encoding\n",
    "def fix_encoding(text):\n",
    "    if pd.isna(text) or text.strip() == \"\":\n",
    "        return text\n",
    "    return ftfy.fix_text(text)\n",
    "\n",
    "metadata['title'] = metadata['title'].apply(fix_encoding)\n",
    "metadata['abstract'] = metadata['abstract'].apply(fix_encoding)\n",
    "metadata['authors'] = metadata['authors'].apply(fix_encoding)\n",
    "metadata['journal'] = metadata['journal'].apply(fix_encoding)\n",
    "\n",
    "# Language detection\n",
    "def is_english(text):\n",
    "    if pd.isna(text) or text.strip() == \"\":\n",
    "        return True\n",
    "    try:\n",
    "        return detect(text) == 'en'\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "metadata['title_is_en'] = metadata['title'].apply(is_english)\n",
    "metadata['abstract_is_en'] = metadata['abstract'].apply(is_english)\n",
    "metadata = metadata[~(metadata['title_is_en'] == False) & ~(metadata['abstract_is_en'] == False)]\n",
    "english_rows = len(metadata)\n",
    "\n",
    "# Clean text with whitelist\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    text = unicodedata.normalize('NFC', text)\n",
    "    # Handle common LaTeX (optional)\n",
    "    text = re.sub(r'\\$\\s*\\\\alpha\\s*\\$', 'alpha', text, flags=re.IGNORECASE)\n",
    "    # Whitelist: a-z, A-Z, 0-9, space, period, comma, hyphen, colon\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s.,-:;/()]', '', text)\n",
    "    # Optional: lowercase now\n",
    "    # text = text.lower()\n",
    "    return text.strip()\n",
    "\n",
    "metadata['title'] = metadata['title'].apply(clean_text)\n",
    "metadata['abstract'] = metadata['abstract'].apply(clean_text)\n",
    "metadata['authors'] = metadata['authors'].apply(clean_text)\n",
    "metadata['journal'] = metadata['journal'].apply(clean_text)\n",
    "\n",
    "# Standardize datetime\n",
    "metadata['publish_time'] = pd.to_datetime(metadata['publish_time'], errors='coerce')\n",
    "\n",
    "# Replace missing abstracts with title\n",
    "metadata['abstract'] = metadata['abstract'].fillna(metadata['title'])\n",
    "\n",
    "# Drop temp columns\n",
    "metadata = metadata.drop(columns=['title_is_en', 'abstract_is_en'])\n",
    "\n",
    "# Log stats\n",
    "print(f\"Original rows: {original_rows}\")\n",
    "print(f\"Rows after English filter: {english_rows}\")\n",
    "print(f\"Rows after cleaning: {len(metadata)}\")\n",
    "\n",
    "# Save\n",
    "metadata.to_csv('metadata_cut_clean.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f31b02b",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63cd7248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess text (lowercase and remove extra spaces)\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):\n",
    "        return ' '.join(text.lower().split())\n",
    "    return text\n",
    "\n",
    "# preprocess metadata file\n",
    "# open file if not already open\n",
    "file_path = \"metadata_cut_clean.csv\"\n",
    "metadata = pd.read_csv(file_path)\n",
    "\n",
    "for col in metadata.select_dtypes(include='object').columns:\n",
    "    metadata[col] = metadata[col].apply(preprocess_text)\n",
    "\n",
    "# remove 'abstract' if first word in abstract col\n",
    "for row in metadata.iterrows():\n",
    "    if metadata.loc[row[0]]['abstract'][:8] == 'abstract':\n",
    "        metadata.loc[row[0], 'abstract'] = metadata.loc[row[0]]['abstract'][9:]\n",
    "    \n",
    "# Save the cleaned data to a new CSV file\n",
    "output_file_path = \"preprocessed_metadata.csv\"\n",
    "metadata.to_csv(output_file_path, index=False)\n",
    "\n",
    "# preprocess topics / queries file\n",
    "# Load the original CSV file\n",
    "file_path = \"topics-rnd3.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    " \n",
    "# Apply preprocessing to all text columns\n",
    "for col in df.select_dtypes(include='object').columns:\n",
    "    df[col] = df[col].apply(preprocess_text)\n",
    "\n",
    "# Save the cleaned data to a new CSV file\n",
    "output_file_path = \"preprocessed_topics.csv\"\n",
    "df.to_csv(output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11bcb87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # lowercase text\n",
    "\n",
    "# metadata['title'] = metadata['title'].str.lower()\n",
    "# metadata['abstract'] = metadata['abstract'].str.lower()\n",
    "\n",
    "\n",
    "\n",
    "# # optional - remove stopwords\n",
    "\n",
    "# from nltk.corpus import stopwords\n",
    "# # import nltk\n",
    "# # nltk.download('stopwords')\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "# metadata['abstract_no_stop'] = metadata['abstract'].apply(\n",
    "#     lambda x: ' '.join(word for word in x.split() if word not in stop_words)\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
